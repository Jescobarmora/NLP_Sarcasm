{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jescobarmora/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import language_tool_python\n",
    "import spacy\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "import re\n",
    "import nltk\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import fasttext\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "train_df_path = os.path.join(parent_dir, 'data', 'Sarcasmo_train.csv')\n",
    "test_df_path = os.path.join(parent_dir, 'data', 'Sarcasmo_test.csv')\n",
    "\n",
    "train_df = pd.read_csv(train_df_path, sep=';', encoding='utf-8')\n",
    "test_df = pd.read_csv(train_df_path, sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Locutor</th>\n",
       "      <th>Locución</th>\n",
       "      <th>Sarcasmo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>archer</td>\n",
       "      <td>No, era por saber si tenía que llevar un saco ...</td>\n",
       "      <td>Si</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>archer</td>\n",
       "      <td>A quién le importa? No, lo pregunto de verdad</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>archer</td>\n",
       "      <td>Pero voy a dar por hecho que ha pedido refuerzos</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>malory</td>\n",
       "      <td>¿Por qué no te callas? Tengo que pensar</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>slater</td>\n",
       "      <td>Sí, sospechábamos un poco</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Locutor                                           Locución Sarcasmo\n",
       "0  archer  No, era por saber si tenía que llevar un saco ...       Si\n",
       "1  archer      A quién le importa? No, lo pregunto de verdad       No\n",
       "2  archer   Pero voy a dar por hecho que ha pedido refuerzos       No\n",
       "3  malory            ¿Por qué no te callas? Tengo que pensar       No\n",
       "4  slater                          Sí, sospechábamos un poco       No"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Locutor</th>\n",
       "      <th>Locución</th>\n",
       "      <th>Sarcasmo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>archer</td>\n",
       "      <td>No, era por saber si tenía que llevar un saco ...</td>\n",
       "      <td>Si</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>archer</td>\n",
       "      <td>A quién le importa? No, lo pregunto de verdad</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>archer</td>\n",
       "      <td>Pero voy a dar por hecho que ha pedido refuerzos</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>malory</td>\n",
       "      <td>¿Por qué no te callas? Tengo que pensar</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>slater</td>\n",
       "      <td>Sí, sospechábamos un poco</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Locutor                                           Locución Sarcasmo\n",
       "0  archer  No, era por saber si tenía que llevar un saco ...       Si\n",
       "1  archer      A quién le importa? No, lo pregunto de verdad       No\n",
       "2  archer   Pero voy a dar por hecho que ha pedido refuerzos       No\n",
       "3  malory            ¿Por qué no te callas? Tengo que pensar       No\n",
       "4  slater                          Sí, sospechábamos un poco       No"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading LanguageTool 6.4: 100%|██████████| 246M/246M [00:05<00:00, 43.9MB/s] \n",
      "Unzipping /tmp/tmpmh9k7puw.zip to /home/jescobarmora/.cache/language_tool_python.\n",
      "Downloaded https://www.languagetool.org/download/LanguageTool-6.4.zip to /home/jescobarmora/.cache/language_tool_python.\n"
     ]
    }
   ],
   "source": [
    "# Inicializar herramienta de corrección ortográfica\n",
    "tool = language_tool_python.LanguageTool('es')\n",
    "\n",
    "def correct_text(text):\n",
    "    matches = tool.check(text)\n",
    "    corrected = language_tool_python.utils.correct(text, matches)\n",
    "    return corrected\n",
    "\n",
    "# Aplicar corrección ortográfica\n",
    "train_df['Locución'] = train_df['Locución'].apply(correct_text)\n",
    "test_df['Locución'] = test_df['Locución'].apply(correct_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo de spaCy en español\n",
    "nlp = spacy.load('es_core_news_lg')\n",
    "\n",
    "# Unir stopwords de NLTK y spaCy\n",
    "stop_nltk = stopwords.words('spanish')\n",
    "stop_spacy = list(STOP_WORDS)\n",
    "stop_words = set(stop_nltk + stop_spacy)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower() for token in doc if token.is_alpha and token.text.lower() not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Aplicar preprocesamiento\n",
    "train_df['processed_text'] = train_df['Locución'].apply(preprocess_text)\n",
    "test_df['processed_text'] = test_df['Locución'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pos_counts(text):\n",
    "    doc = nlp(text)\n",
    "    pos_counts = doc.count_by(spacy.attrs.POS)\n",
    "    total_tokens = len(doc)\n",
    "    features = {\n",
    "        'nouns': pos_counts.get(nlp.vocab.strings['NOUN'], 0) / total_tokens,\n",
    "        'verbs': pos_counts.get(nlp.vocab.strings['VERB'], 0) / total_tokens,\n",
    "        'adjectives': pos_counts.get(nlp.vocab.strings['ADJ'], 0) / total_tokens,\n",
    "        'adverbs': pos_counts.get(nlp.vocab.strings['ADV'], 0) / total_tokens,\n",
    "        # Agrega más categorías si lo deseas\n",
    "    }\n",
    "    return pd.Series(features)\n",
    "\n",
    "# Aplicar extracción de características\n",
    "train_pos_features = train_df['Locución'].apply(extract_pos_counts)\n",
    "test_pos_features = test_df['Locución'].apply(extract_pos_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6855139e8194832ade8e4d3a44ab97c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/364 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eafc547c0c5f4a37bd7398680d920060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/648 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc8187fe51f42b68a61fef646acceab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/242k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a7a13bcd6440009338db40e189896b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/480k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b543b58db2646d1b4f18f8fa566900c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/134 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e078ad825a4673b8929ff8b47d5ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa09dbc8e88414cb19bb48e3818257d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')\n",
    "model = AutoModel.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')\n",
    "\n",
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "    outputs = model(**inputs)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :].detach().numpy()\n",
    "    return cls_embedding.flatten()\n",
    "\n",
    "# Obtener embeddings\n",
    "train_df['bert_embedding'] = train_df['processed_text'].apply(get_bert_embedding)\n",
    "test_df['bert_embedding'] = test_df['processed_text'].apply(get_bert_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  119\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  258126 lr:  0.000000 avg.loss:  4.128316 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Guardar textos en archivos temporales\n",
    "train_df['processed_text'].to_csv('train_texts.txt', index=False, header=False)\n",
    "test_df['processed_text'].to_csv('test_texts.txt', index=False, header=False)\n",
    "\n",
    "# Entrenar modelo FastText\n",
    "ft_model = fasttext.train_unsupervised('train_texts.txt', model='skipgram')\n",
    "\n",
    "def get_fasttext_embedding(text):\n",
    "    words = text.split()\n",
    "    word_embeddings = [ft_model.get_word_vector(word) for word in words if word in ft_model.words]\n",
    "    if len(word_embeddings) == 0:\n",
    "        return np.zeros(ft_model.get_dimension())\n",
    "    else:\n",
    "        return np.mean(word_embeddings, axis=0)\n",
    "\n",
    "# Obtener embeddings\n",
    "train_df['fasttext_embedding'] = train_df['processed_text'].apply(get_fasttext_embedding)\n",
    "test_df['fasttext_embedding'] = test_df['processed_text'].apply(get_fasttext_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificar etiquetas\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['label'] = label_encoder.fit_transform(train_df['Sarcasmo'])\n",
    "test_df['label'] = label_encoder.transform(test_df['Sarcasmo'])\n",
    "\n",
    "# Convertir listas de embeddings a matrices\n",
    "train_bert_embeddings = np.stack(train_df['bert_embedding'].values)\n",
    "test_bert_embeddings = np.stack(test_df['bert_embedding'].values)\n",
    "\n",
    "train_fasttext_embeddings = np.stack(train_df['fasttext_embedding'].values)\n",
    "test_fasttext_embeddings = np.stack(test_df['fasttext_embedding'].values)\n",
    "\n",
    "# Convertir características gramaticales a matrices\n",
    "train_pos_features = train_pos_features.reset_index(drop=True)\n",
    "test_pos_features = test_pos_features.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_probs = model.predict_proba(X_test)[:, 1]\n",
    "    roc_auc = roc_auc_score(y_test, y_probs)\n",
    "    accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "    return roc_auc, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo 1: BERT sin Ingeniería de Características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo 1 - BERT sin ingeniería de características: ROC AUC = 0.9929, Accuracy = 0.9970\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Entrenar modelo\n",
    "model1 = LogisticRegression(max_iter=1000)\n",
    "model1.fit(train_bert_embeddings, train_df['label'])\n",
    "\n",
    "# Evaluar modelo\n",
    "roc_auc1, accuracy1 = evaluate_model(model1, test_bert_embeddings, test_df['label'])\n",
    "print(f'Modelo 1 - BERT sin ingeniería de características: ROC AUC = {roc_auc1:.4f}, Accuracy = {accuracy1:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo 2: BERT con Ingeniería de Características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo 2 - BERT con ingeniería de características: ROC AUC = 0.9996, Accuracy = 0.9985\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Concatenar embeddings y características\n",
    "X_train_model2 = np.hstack([train_bert_embeddings, train_pos_features.values])\n",
    "X_test_model2 = np.hstack([test_bert_embeddings, test_pos_features.values])\n",
    "\n",
    "# Escalar características\n",
    "scaler = StandardScaler()\n",
    "X_train_model2 = scaler.fit_transform(X_train_model2)\n",
    "X_test_model2 = scaler.transform(X_test_model2)\n",
    "\n",
    "# Entrenar modelo\n",
    "model2 = LogisticRegression(max_iter=1000)\n",
    "model2.fit(X_train_model2, train_df['label'])\n",
    "\n",
    "# Evaluar modelo\n",
    "roc_auc2, accuracy2 = evaluate_model(model2, X_test_model2, test_df['label'])\n",
    "print(f'Modelo 2 - BERT con ingeniería de características: ROC AUC = {roc_auc2:.4f}, Accuracy = {accuracy2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo 3: FastText sin Ingeniería de Características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo 3 - FastText sin ingeniería de características: ROC AUC = 0.7176, Accuracy = 0.9285\n"
     ]
    }
   ],
   "source": [
    "# Entrenar modelo\n",
    "model3 = LogisticRegression(max_iter=1000)\n",
    "model3.fit(train_fasttext_embeddings, train_df['label'])\n",
    "\n",
    "# Evaluar modelo\n",
    "roc_auc3, accuracy3 = evaluate_model(model3, test_fasttext_embeddings, test_df['label'])\n",
    "print(f'Modelo 3 - FastText sin ingeniería de características: ROC AUC = {roc_auc3:.4f}, Accuracy = {accuracy3:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo 4 - FastText con ingeniería de características: ROC AUC = 0.8873, Accuracy = 0.9434\n"
     ]
    }
   ],
   "source": [
    "# Concatenar embeddings y características\n",
    "X_train_model4 = np.hstack([train_fasttext_embeddings, train_pos_features.values])\n",
    "X_test_model4 = np.hstack([test_fasttext_embeddings, test_pos_features.values])\n",
    "\n",
    "# Escalar características\n",
    "X_train_model4 = scaler.fit_transform(X_train_model4)\n",
    "X_test_model4 = scaler.transform(X_test_model4)\n",
    "\n",
    "# Entrenar modelo\n",
    "model4 = LogisticRegression(max_iter=1000)\n",
    "model4.fit(X_train_model4, train_df['label'])\n",
    "\n",
    "# Evaluar modelo\n",
    "roc_auc4, accuracy4 = evaluate_model(model4, X_test_model4, test_df['label'])\n",
    "print(f'Modelo 4 - FastText con ingeniería de características: ROC AUC = {roc_auc4:.4f}, Accuracy = {accuracy4:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Modelo   ROC AUC  Accuracy\n",
      "1      BERT con ingeniería de características  0.999632  0.998510\n",
      "0      BERT sin ingeniería de características  0.992894  0.997019\n",
      "3  FastText con ingeniería de características  0.887256  0.943368\n",
      "2  FastText sin ingeniería de características  0.717596  0.928465\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Crear DataFrame con los resultados\n",
    "results = pd.DataFrame({\n",
    "    'Modelo': [\n",
    "        'BERT sin ingeniería de características',\n",
    "        'BERT con ingeniería de características',\n",
    "        'FastText sin ingeniería de características',\n",
    "        'FastText con ingeniería de características'\n",
    "    ],\n",
    "    'ROC AUC': [roc_auc1, roc_auc2, roc_auc3, roc_auc4],\n",
    "    'Accuracy': [accuracy1, accuracy2, accuracy3, accuracy4]\n",
    "})\n",
    "\n",
    "# Ordenar por ROC AUC\n",
    "results = results.sort_values(by='ROC AUC', ascending=False)\n",
    "print(results)\n",
    "\n",
    "# Guardar resultados en CSV\n",
    "results.to_csv('resultados_modelos.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "api_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
